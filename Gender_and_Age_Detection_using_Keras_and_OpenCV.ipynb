{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfS7dX+1LmEpe/Nr/Il6BG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahtesham-Ibne-Mostafa/Image_Processing/blob/main/Gender_and_Age_Detection_using_Keras_and_OpenCV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5IStpi9lQ8W"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/TechVidvanâ€™)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "UC4CvSZ6lZ52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load data\n",
        "fold0 = pd.read_csv(\"AdienceBenchmarkGenderAndAgeClassification/fold_0_data.txt\",sep = \"\\t\" )\n",
        "fold1 = pd.read_csv(\"AdienceBenchmarkGenderAndAgeClassification/fold_1_data.txt\",sep = \"\\t\")\n",
        "fold2 = pd.read_csv(\"AdienceBenchmarkGenderAndAgeClassification/fold_2_data.txt\",sep = \"\\t\")\n",
        "fold3 = pd.read_csv(\"AdienceBenchmarkGenderAndAgeClassification/fold_3_data.txt\",sep = \"\\t\")\n",
        "fold4 = pd.read_csv(\"AdienceBenchmarkGenderAndAgeClassification/fold_4_data.txt\",sep = \"\\t\")"
      ],
      "metadata": {
        "id": "oWMrMcRwle77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_data = pd.concat([fold0, fold1, fold2, fold3, fold4], ignore_index=True)\n",
        "print(total_data.shape)\n",
        "total_data.info()"
      ],
      "metadata": {
        "id": "NYCy2fWuliB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_data.head()"
      ],
      "metadata": {
        "id": "WhwVRBpWli27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bar chart\n",
        "gender = ['f','m','u']\n",
        "plt.bar(gender, total_data.gender.value_counts(), align='center', alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_B76YUvFlkhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense, Dropout, LayerNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img"
      ],
      "metadata": {
        "id": "TYp_98ZQll98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"AdienceBenchmarkGenderAndAgeClassification/faces/\"+total_data.user_id.loc[0]+\"/coarse_tilt_aligned_face.\"+str(total_data.face_id.loc[0])+\".\"+total_data.original_image.loc[0]\n",
        "img = load_img(path)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YHUpBqKKlnyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imp_data = total_data[['age', 'gender', 'x', 'y', 'dx', 'dy']].copy()\n",
        "imp_data.info()\n",
        "\n",
        "img_path = []\n",
        "for row in total_data.iterrows():\n",
        "    path = \"AdienceBenchmarkGenderAndAgeClassification/faces/\"+row[1].user_id+\"/coarse_tilt_aligned_face.\"+str(row[1].face_id)+\".\"+row[1].original_image\n",
        "    img_path.append(path)\n",
        "\n",
        "imp_data['img_path'] = img_path\n",
        "imp_data.head()"
      ],
      "metadata": {
        "id": "TU20HrL1lp5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_mapping = [('(0, 2)', '0-2'), ('2', '0-2'), ('3', '0-2'), ('(4, 6)', '4-6'), ('(8, 12)', '8-13'), ('13', '8-13'), ('22', '15-20'), ('(8, 23)','15-20'), ('23', '25-32'), ('(15, 20)', '15-20'), ('(25, 32)', '25-32'), ('(27, 32)', '25-32'), ('32', '25-32'), ('34', '25-32'), ('29', '25-32'), ('(38, 42)', '38-43'), ('35', '38-43'), ('36', '38-43'), ('42', '48-53'), ('45', '38-43'), ('(38, 43)', '38-43'), ('(38, 42)', '38-43'), ('(38, 48)', '48-53'), ('46', '48-53'), ('(48, 53)', '48-53'), ('55', '48-53'), ('56', '48-53'), ('(60, 100)', '60+'), ('57', '60+'), ('58', '60+')]\n",
        "\n",
        "age_mapping_dict = {each[0]: each[1] for each in age_mapping}\n",
        "drop_labels = []\n",
        "for idx, each in enumerate(imp_data.age):\n",
        "    if each == 'None':\n",
        "        drop_labels.append(idx)\n",
        "    else:\n",
        "        imp_data.age.loc[idx] = age_mapping_dict[each]\n",
        "\n",
        "imp_data = imp_data.drop(labels=drop_labels, axis=0) #droped None values\n",
        "imp_data.age.value_counts(dropna=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "KPW3mI_Alrw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imp_data = imp_data.dropna()\n",
        "clean_data = imp_data[imp_data.gender != 'u'].copy()\n",
        "clean_data.info()"
      ],
      "metadata": {
        "id": "5iKYhb-7luZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_to_label_map = {\n",
        "    'f' : 0,\n",
        "    'm' : 1\n",
        "}\n",
        "\n",
        "clean_data['gender'] = clean_data['gender'].apply(lambda g: gender_to_label_map[g])\n",
        "clean_data.head()"
      ],
      "metadata": {
        "id": "cyh_0yLylvyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_to_label_map = {\n",
        "    '0-2'  :0,\n",
        "    '4-6'  :1,\n",
        "    '8-13' :2,\n",
        "    '15-20':3,\n",
        "    '25-32':4,\n",
        "    '38-43':5,\n",
        "    '48-53':6,\n",
        "    '60+'  :7\n",
        "}\n",
        "\n",
        "clean_data['age'] = clean_data['age'].apply(lambda age: age_to_label_map[age])\n",
        "clean_data.head()"
      ],
      "metadata": {
        "id": "tAixi-RmlxVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = clean_data[['img_path']]\n",
        "y = clean_data[['gender']]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print('Train data shape {}'.format(X_train.shape))\n",
        "print('Test data shape {}'.format(X_test.shape))\n",
        "\n",
        "train_images = []\n",
        "test_images = []\n",
        "\n",
        "for row in X_train.iterrows():\n",
        "    image = Image.open(row[1].img_path)\n",
        "    image = image.resize((227, 227))   # Resize the image\n",
        "    data = np.asarray(image)\n",
        "    train_images.append(data)\n",
        "\n",
        "for row in X_test.iterrows():\n",
        "    image = Image.open(row[1].img_path)\n",
        "    image = image.resize((227, 227))  # Resize the image\n",
        "    data = np.asarray(image)\n",
        "    test_images.append(data)\n",
        "\n",
        "train_images = np.asarray(train_images)\n",
        "test_images = np.asarray(test_images)\n",
        "\n",
        "print('Train images shape {}'.format(train_images.shape))\n",
        "print('Test images shape {}'.format(test_images.shape))"
      ],
      "metadata": {
        "id": "t30OO0hJlzKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(227, 227, 3), filters=96, kernel_size=(7, 7), strides=4, padding='valid', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(LayerNormalization())\n",
        "model.add(Conv2D(filters=256, kernel_size=(5, 5), strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(LayerNormalization())\n",
        "model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(LayerNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=512, activation='relu'))\n",
        "model.add(Dropout(rate=0.25))\n",
        "model.add(Dense(units=512, activation='relu'))\n",
        "model.add(Dropout(rate=0.25))\n",
        "model.add(Dense(units=2, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "GHtMzEm7l10L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3) # Callback for earlystopping\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "history = model.fit(train_images, y_train, batch_size=32, epochs=25, validation_data=(test_images, y_test), callbacks=[callback])\n",
        "\n",
        "print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "\n",
        "model.save('gender_model25.h5')"
      ],
      "metadata": {
        "id": "qz7-su9Al33W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = clean_data[['img_path']]\n",
        "y = clean_data[['age']]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print('Train data shape {}'.format(X_train.shape))\n",
        "print('Test data shape {}'.format(X_test.shape))\n",
        "\n",
        "train_images = []\n",
        "test_images = []\n",
        "\n",
        "for row in X_train.iterrows():\n",
        "    image = Image.open(row[1].img_path)\n",
        "    image = image.resize((227, 227))   # Resize the image\n",
        "    data = np.asarray(image)\n",
        "    train_images.append(data)\n",
        "\n",
        "for row in X_test.iterrows():\n",
        "    image = Image.open(row[1].img_path)\n",
        "    image = image.resize((227, 227))  # Resize the image\n",
        "    data = np.asarray(image)\n",
        "    test_images.append(data)\n",
        "\n",
        "train_images = np.asarray(train_images)\n",
        "test_images = np.asarray(test_images)\n",
        "\n",
        "print('Train images shape {}'.format(train_images.shape))\n",
        "print('Test images shape {}'.format(test_images.shape))\n"
      ],
      "metadata": {
        "id": "qKyRCPSNl566"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(227, 227, 3), filters=96, kernel_size=(7, 7), strides=4, padding='valid', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(LayerNormalization())\n",
        "model.add(Conv2D(filters=256, kernel_size=(5, 5), strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(LayerNormalization())\n",
        "model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(LayerNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=512, activation='relu'))\n",
        "model.add(Dropout(rate=0.25))\n",
        "model.add(Dense(units=512, activation='relu'))\n",
        "model.add(Dropout(rate=0.25))\n",
        "model.add(Dense(units=8, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "avy0VM7ll8XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3) # Callback for earlystopping\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "history = model.fit(train_images, y_train, batch_size=32, epochs=50, validation_data=(test_images, y_test), callbacks=[callback])\n",
        "\n",
        "model.save('age_model50.h5')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, y_test, verbose=2)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "id": "GnSkGk5Tl-zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import json\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input\n",
        "import numpy as np\n",
        "import argparse\n",
        "from wide_resnet import WideResNet\n",
        "from keras.utils.data_utils import get_file\n",
        "import face_recognition"
      ],
      "metadata": {
        "id": "gHSan_H0mA-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_model = tf.keras.models.load_model('weights.hdf5')\n",
        "gender_model.summary()"
      ],
      "metadata": {
        "id": "xN15Jh27mDOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_map=[['0-2'],['4-6'],['8-13'],['15-20'],['25-32'],['38-43'],['48-63'],['60+']]"
      ],
      "metadata": {
        "id": "g-vl8AHQmGK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_face(self):\n",
        "    cap=cv2.VideoCapture(0)\n",
        "    while True:\n",
        "        grb,frame=cap.read()\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        if not grb:\n",
        "            break"
      ],
      "metadata": {
        "id": "DGy5UItqmI_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face_locations = face_recognition.face_locations(frame)\n",
        "print(face_locations)\n",
        "if(face_locations==[]):\n",
        "    cv2.imshow('Gender and age', frame)\n",
        "    if cv2.waitKey(1) == 27:\n",
        "        break\n",
        "else:\n",
        "    cv2.rectangle(frame, (face_locations[0][3], face_locations[0][0]), (face_locations[0][1], face_locations[0][2]), (255, 200, 0), 2)\n",
        "    img=frame[face_locations[0][0]-25: face_locations[0][2]+25, face_locations[0][3]-25: face_locations[0][1]+25]"
      ],
      "metadata": {
        "id": "tY7xG9DMmKfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict ages and genders of the detected faces\n",
        "img2= cv2.resize(img, (64, 64))\n",
        "img2=np.array([img2]).reshape((1, 64,64,3))\n",
        "results = self.model.predict(img2)"
      ],
      "metadata": {
        "id": "W3JFrmclmLTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_genders = results[0]\n",
        "gen=\"F\" if predicted_genders[0][0] > 0.5 else \"M\"\n",
        "ages = np.arange(0, 101).reshape(101, 1)\n",
        "predicted_ages = results[1].dot(ages).flatten()"
      ],
      "metadata": {
        "id": "Pl1dUZu-mQvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred=\"\"\n",
        "pred=str(int(predicted_ages[0]))+\" \"+str(gen)\n",
        "print(pred)\n",
        "cv2.putText(frame, pred,(face_locations[0][3],face_locations[0][0]) , cv2.FONT_HERSHEY_SIMPLEX,0.7, (2, 255, 255), 2)\n",
        "\n",
        "        cv2.imshow('Gender and age', frame)\n",
        "        if cv2.waitKey(1) == 27:\n",
        "            break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "# When everything is done, release the capture"
      ],
      "metadata": {
        "id": "ezghvcSjmTSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GBd7dtAHmUKI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}